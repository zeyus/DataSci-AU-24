{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 4 - Bias-variance trade-off and Decision Trees\n",
    "Yesterday, we introduce the notions of bias and variance, which quantify two dimensions of error in predictive modeling. We saw that:\n",
    "- *Bias* is the distance between the average of predictions made by a given model when trained on different training sets and the predictions made by the true model;\n",
    "- *Variance* is the variance of predictions made by a given model when trained on different training sets.\n",
    "\n",
    "Importantly, bias and variance are modulated by the complexity or flexibility of a model in different ways. As complexity/flexibility increases, bias tends to decrease, while variance increases.\n",
    "\n",
    "This is why we talk about a _bias-variance trade-off_: ideally, we want to minimize both sources of prediction error, but in practice, modeling decisions that lead to lower bias will also lead to higher variance (and viceversa).\n",
    "What we want to do when designing our model is to hit the optimal balance between bias and variance.\n",
    "\n",
    "Alongside this, we introduced **decision trees** as an additional modeling algorithm for both regression and classification.\n",
    "Decision trees are implemented in `scikit-learn`. You can check out this link for a brush-up of what they are with some examples: https://scikit-learn.org/stable/modules/tree.html. \n",
    "Classes for regression (`DecisionTreeRegressor`) and classification (`DecisionTreeClassifier`) are documented respectively at: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html and https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html. \n",
    "We have discussed hyperparameters of decision trees that influence its complexity and flexibility (e.g., tree depth, minimum number of total samples per leaf), and discussed why flexibility matters for learning good decision trees.\n",
    "\n",
    "In this exercise, we will take a break from our attempt to model bike data (fear not, we will resume next week and combine all we have learned so far with tree-based bagging and boosting methods, which we will introduce next week).\n",
    "Our goal for today is see some of the notions we have introduced theoretically during our lecture in action, to get a better grasp of these complex notions.\n",
    "We will look at a practical example of how bias and variance are modulated by model flexibility, using a linear model and decision trees as a test case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Today's plan\n",
    "**Step 1** - Generating Data:\n",
    "1. Generate 1000 data points from a uniform distribution, using `numpy.random.uniform` (https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html). Set the minimum value to -10 and the maximum value to 10. This will be your $x$.\n",
    "2. Generate polynomial terms for x, using `sklearn.preprocessing.PolynomialFeatures` (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "3. Generate 4 random coefficients $\\beta_{i}$ from a uniform distribution (same parameters as above)\n",
    "4. Generate an outcome variable using the following equation: $y = \\beta_{0} + \\beta_{1} * x + \\beta_{2} * x^2 + \\beta_{3} * x^3 $\n",
    "5. Add some Gaussian noise to your $y$ using `np.random.normal` (set the `scale` parameter to 10 to make this noise \"visible\", see https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html)\n",
    "6. Plot the relation between $x$ and $y$\n",
    "7. Finally, split your data into two halves, a training and a test set, using sklearn's `train_test_split`\n",
    "\n",
    "**Step 2** - Compute squared bias and variance of linear models with increasing complexity:\n",
    "1. Compute average **squared bias**, **variance**, and **MSE** for a linear model that only uses *x* as input feature.\n",
    "    - To do so, you need to create a loop where you take random samples of the training data (1/2 the size of the training set), say, 100 times. This process is called **bootstrapping**.\n",
    "    - For each random split of the data:\n",
    "        - You fit a `LinearRegression` estimator on the sampled data\n",
    "        - You predict $\\hat{y}$ for all test data points\n",
    "    - Then you compute the average **squared** bias across all data points in the test set. To do so you need to:\n",
    "            1. Compute the average prediction of your bootstrapped models for each data point\n",
    "            2. Compute the difference between these values and the predictions of the true model for each data point\n",
    "            3. Take the squared value of that\n",
    "            4. Average across all data points\n",
    "            5. NOTE: we look at the average *squared* bias, because the bias of a single data point encodes information about the *directionality* of the error. If we averaged those values, we will underestimate models' tendency to make systematic errors.\n",
    "    - Then you compute the average variance of the predictions of bootstrapped models across all data points in the test set\n",
    "    - Then you compute the average MSE of your method (computing the average of MSEs of each bootstrapped model)\n",
    "2. Now that you know how to compute this, scale this up. Fit linear regressions with input features that include increasingly high polynomial expansions of your feature set (first only $x, x^2$, then $x, x^2, x^3$ and all the way up to $x,...,x^{10}$)\n",
    "3. Make a plot where you show how bias, variance and MSE change as a function of model complexity (i.e., how many polynomials you include)\n",
    "4. Look at the patterns emerging from the plot and discuss them with your group: what do you notice?\n",
    "\n",
    "**Bonus tasks**:\n",
    "- We have talked about decision trees in the lecture. Can you do minimal changes to the code above, to look at how bias and variance change as a function of the expected depth of a tree, or additional parameters related to the complexity of a decision tree? Use `DecisionTreeRegressor` (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) as an extractor, and loop over multiple values of `max_depth` (e.g., ranging from 1 to 20)\n",
    "- How do your results change as you manipulate parameters such as the the true generative model (e.g., its complexity), the size of bootstrapped data samples, etc?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "As usual, you will find a solution in `example.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
